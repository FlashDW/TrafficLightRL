normalized observations and rewards


changed actions:
only controls the duration. cycle is built in.

changed obs to:
[current light state, time remaining in state, number of cars in each lane (4)]

changed durations to:
[1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20]


model: PPO

trained on GPU of RTX 4000

trained with 8 envs

10 million timesteps

neural network: 256, 256, 128

ReLU

learning rate scheduele: lr_schedule = lambda progress_remaining: max(5e-4 * progress_remaining, 1e-5)

n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        vf_coef=0.5,
        max_grad_norm=0.5,
        policy_kwargs=policy_kwargs,

Training took 13,231 seconds
or
221 minutes
or
3.7 hours

Average crashes per minute: 2.7
Average wait time per car: 6 seconds